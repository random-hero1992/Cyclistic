{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2269d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea737d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4073561, 13)\n"
     ]
    }
   ],
   "source": [
    "# Read .csv files into individual dataframes\n",
    "ds1=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202006-divvy-tripdata.csv', header=True)\n",
    "ds2=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202007-divvy-tripdata.csv', header=True)\n",
    "ds3=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202008-divvy-tripdata.csv', header=True)\n",
    "ds4=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202009-divvy-tripdata.csv', header=True)\n",
    "ds5=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202010-divvy-tripdata.csv', header=True)\n",
    "ds6=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202011-divvy-tripdata.csv', header=True)\n",
    "ds7=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202012-divvy-tripdata.csv', header=True)\n",
    "ds8=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202101-divvy-tripdata.csv', header=True)\n",
    "ds9=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202102-divvy-tripdata.csv', header=True)\n",
    "ds10=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202103-divvy-tripdata.csv', header=True)\n",
    "ds11=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202104-divvy-tripdata.csv', header=True)\n",
    "ds12=spark.read.csv(r'C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\202105-divvy-tripdata.csv', header=True)\n",
    "\n",
    "# Merge all datasets to one dataframe\n",
    "ds=ds1.union(ds2).union(ds3).union(ds4).union(ds5).union(ds6).union(ds7).union(ds8).union(ds9).union(ds10).union(ds11).union(ds12)\n",
    "\n",
    "# Print number of records and columns\n",
    "print((ds.count(), len(ds.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcf1913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ride_id='8CD5DE2C2B6C4CFC', rideable_type='docked_bike', started_at='2020-06-13 23:24:48', ended_at='2020-06-13 23:36:55', start_station_name='Wilton Ave & Belmont Ave', start_station_id='117', end_station_name='Damen Ave & Clybourn Ave', end_station_id='163', start_lat='41.94018', start_lng='-87.65304', end_lat='41.931931', end_lng='-87.677856', member_casual='casual')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first row of data to have a peek\n",
    "ds.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b46ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|start_lat| start_lng|  end_lat|   end_lng|member_casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "|8CD5DE2C2B6C4CFC|  docked_bike|2020-06-13 23:24:48|2020-06-13 23:36:55|Wilton Ave & Belm...|             117|Damen Ave & Clybo...|           163| 41.94018| -87.65304|41.931931|-87.677856|       casual|\n",
      "|9A191EB2C751D85D|  docked_bike|2020-06-26 07:26:10|2020-06-26 07:31:58|Federal St & Polk St|              41|  Daley Center Plaza|            81|41.872077|-87.629543|41.884241|-87.629634|       member|\n",
      "|F37D14B0B5659BCF|  docked_bike|2020-06-23 17:12:41|2020-06-23 17:21:14|  Daley Center Plaza|              81|State St & Harris...|             5|41.884241|-87.629634|41.874053|-87.627716|       member|\n",
      "|C41237B506E85FA1|  docked_bike|2020-06-20 01:09:35|2020-06-20 01:28:24|Broadway & Cornel...|             303|Broadway & Berwyn...|           294|41.945529|-87.646439|41.978353|-87.659753|       casual|\n",
      "|4B51B3B0BDA7787C|  docked_bike|2020-06-25 16:59:25|2020-06-25 17:08:48|Sheffield Ave & W...|             327|Wilton Ave & Belm...|           117| 41.92154|-87.653818| 41.94018| -87.65304|       casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first five rows of data\n",
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbae791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4073561"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate rows and count all rows again\n",
    "ds.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a40fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|start_lat| start_lng|  end_lat|   end_lng|member_casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "|8CD5DE2C2B6C4CFC|  docked_bike|2020-06-13 23:24:48|2020-06-13 23:36:55|Wilton Ave & Belm...|             117|Damen Ave & Clybo...|           163| 41.94018| -87.65304|41.931931|-87.677856|       casual|\n",
      "|9A191EB2C751D85D|  docked_bike|2020-06-26 07:26:10|2020-06-26 07:31:58|Federal St & Polk St|              41|  Daley Center Plaza|            81|41.872077|-87.629543|41.884241|-87.629634|       member|\n",
      "|F37D14B0B5659BCF|  docked_bike|2020-06-23 17:12:41|2020-06-23 17:21:14|  Daley Center Plaza|              81|State St & Harris...|             5|41.884241|-87.629634|41.874053|-87.627716|       member|\n",
      "|C41237B506E85FA1|  docked_bike|2020-06-20 01:09:35|2020-06-20 01:28:24|Broadway & Cornel...|             303|Broadway & Berwyn...|           294|41.945529|-87.646439|41.978353|-87.659753|       casual|\n",
      "|4B51B3B0BDA7787C|  docked_bike|2020-06-25 16:59:25|2020-06-25 17:08:48|Sheffield Ave & W...|             327|Wilton Ave & Belm...|           117| 41.92154|-87.653818| 41.94018| -87.65304|       casual|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+----------+---------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "4073561\n"
     ]
    }
   ],
   "source": [
    "# Clean date fields\n",
    "ds.filter(\"started_at IS NOT NULL and ended_at IS NOT NULL\").show(5)\n",
    "print(ds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1747d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+----------+------------------+\n",
      "|start_lat| start_lng|  end_lat|   end_lng| distance_traveled|\n",
      "+---------+----------+---------+----------+------------------+\n",
      "| 41.94018| -87.65304|41.931931|-87.677856| 2248.317583317838|\n",
      "|41.872077|-87.629543|41.884241|-87.629634|1352.5960705541047|\n",
      "|41.884241|-87.629634|41.874053|-87.627716|1143.9287821509456|\n",
      "|41.945529|-87.646439|41.978353|-87.659753|3812.2639415619205|\n",
      "| 41.92154|-87.653818| 41.94018| -87.65304|2073.6724042245105|\n",
      "+---------+----------+---------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column for distance traveled in meters using start and end lat-lng\n",
    "import pyspark.sql.functions as F\n",
    "ds = ds.withColumn(\"a\", (\n",
    "        F.pow(F.sin(F.radians(F.col(\"end_lat\") - F.col(\"start_lat\")) / 2), 2) +\n",
    "        F.cos(F.radians(F.col(\"start_lat\"))) * F.cos(F.radians(F.col(\"end_lat\"))) *\n",
    "        F.pow(F.sin(F.radians(F.col(\"end_lng\") - F.col(\"start_lng\")) / 2), 2)\n",
    "    )).withColumn(\"distance_traveled\", F.atan2(F.sqrt(F.col(\"a\")), F.sqrt(-F.col(\"a\") + 1)) * 12742000)\n",
    "\n",
    "# View the new column to verify it was successfully created\n",
    "ds.select('start_lat', 'start_lng', 'end_lat', 'end_lng', 'distance_traveled').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6997811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+---------+\n",
      "|         started_at|           ended_at|member_casual|date_diff|\n",
      "+-------------------+-------------------+-------------+---------+\n",
      "|2020-09-02 18:34:33|2020-10-10 11:17:54|       casual|       38|\n",
      "|2021-05-02 02:56:07|2021-06-08 13:37:43|       casual|       37|\n",
      "|2020-09-06 23:20:29|2020-10-12 11:46:25|       casual|       36|\n",
      "|2020-07-05 14:25:39|2020-08-09 07:11:06|       casual|       35|\n",
      "|2020-09-05 08:50:15|2020-10-10 13:43:02|       casual|       35|\n",
      "|2020-07-05 01:51:06|2020-08-08 12:13:19|       casual|       34|\n",
      "|2020-07-07 14:36:11|2020-08-09 19:13:11|       casual|       33|\n",
      "|2020-07-02 19:49:10|2020-08-04 18:00:37|       casual|       33|\n",
      "|2021-04-02 17:50:00|2021-05-05 22:06:42|       casual|       33|\n",
      "|2020-07-02 17:26:55|2020-08-04 07:16:12|       casual|       33|\n",
      "+-------------------+-------------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column that finds the date difference and show it in descending order\n",
    "ds = ds.withColumn('date_diff', F.datediff(F.to_date(ds.ended_at), F.to_date(ds.started_at)))\n",
    "ds.select('started_at', 'ended_at', 'member_casual', 'date_diff').sort(ds.date_diff.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ba1606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+---------+\n",
      "|         started_at|           ended_at|member_casual|date_diff|\n",
      "+-------------------+-------------------+-------------+---------+\n",
      "|2020-12-15 11:39:19|2020-11-25 10:16:59|       member|      -20|\n",
      "|2020-12-15 11:42:13|2020-11-25 10:50:56|       casual|      -20|\n",
      "|2020-12-15 11:37:49|2020-11-25 09:04:08|       casual|      -20|\n",
      "|2020-12-15 12:11:41|2020-11-25 20:03:07|       casual|      -20|\n",
      "|2020-12-15 12:05:43|2020-11-25 20:20:05|       member|      -20|\n",
      "|2020-12-15 12:15:37|2020-11-25 16:38:59|       casual|      -20|\n",
      "|2020-12-15 11:51:59|2020-11-25 14:16:40|       casual|      -20|\n",
      "|2020-12-15 12:12:41|2020-11-25 13:52:21|       member|      -20|\n",
      "|2020-12-15 11:48:35|2020-11-25 13:21:43|       member|      -20|\n",
      "|2020-12-15 11:50:32|2020-11-25 14:06:37|       member|      -20|\n",
      "+-------------------+-------------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order the date difference in ascending order\n",
    "ds.select('started_at', 'ended_at', 'member_casual', 'date_diff').sort(ds.date_diff.asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77db042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4073182"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove records where end date comes before start date\n",
    "ds = ds.filter(col('date_diff').cast(LongType()) >= 0)\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "389b5b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+---------+\n",
      "|         started_at|           ended_at|member_casual|date_diff|\n",
      "+-------------------+-------------------+-------------+---------+\n",
      "|2020-06-28 11:40:46|2020-06-28 11:54:49|       casual|        0|\n",
      "|2020-06-06 14:53:58|2020-06-06 15:18:05|       casual|        0|\n",
      "|2020-06-17 09:55:08|2020-06-17 10:05:52|       casual|        0|\n",
      "|2020-06-14 18:48:05|2020-06-14 20:18:44|       casual|        0|\n",
      "|2020-06-04 17:23:09|2020-06-04 17:50:05|       casual|        0|\n",
      "|2020-06-06 17:44:41|2020-06-06 17:53:51|       casual|        0|\n",
      "|2020-06-17 18:36:11|2020-06-17 18:42:58|       member|        0|\n",
      "|2020-06-28 07:19:49|2020-06-28 08:18:24|       casual|        0|\n",
      "|2020-06-24 21:29:59|2020-06-24 21:48:24|       member|        0|\n",
      "|2020-06-24 21:29:09|2020-06-24 21:29:15|       member|        0|\n",
      "+-------------------+-------------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confirm records were removed\n",
    "# Order the date difference in ascending order\n",
    "ds.select('started_at', 'ended_at', 'member_casual', 'date_diff').sort(ds.date_diff.asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfbec7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+---------+------------------+------------------+\n",
      "|         started_at|           ended_at|member_casual|date_diff| distance_traveled|   duration_in_min|\n",
      "+-------------------+-------------------+-------------+---------+------------------+------------------+\n",
      "|2020-06-13 23:24:48|2020-06-13 23:36:55|       casual|        0| 2248.317583317838|12.116666666666742|\n",
      "|2020-06-26 07:26:10|2020-06-26 07:31:58|       member|        0|1352.5960705541047| 5.799999999999973|\n",
      "|2020-06-23 17:12:41|2020-06-23 17:21:14|       member|        0|1143.9287821509456| 8.550000000000015|\n",
      "|2020-06-20 01:09:35|2020-06-20 01:28:24|       casual|        0|3812.2639415619205|18.816666666666674|\n",
      "|2020-06-25 16:59:25|2020-06-25 17:08:48|       casual|        0|2073.6724042245105| 9.383333333333288|\n",
      "|2020-06-17 18:07:18|2020-06-17 18:18:14|       casual|        0|2073.6724042245105|10.933333333333348|\n",
      "|2020-06-25 07:24:33|2020-06-25 07:31:11|       member|        0|1352.5960705541047| 6.633333333333337|\n",
      "|2020-06-19 00:00:56|2020-06-19 00:09:15|       casual|        0|1151.4433445280395| 8.316666666666666|\n",
      "|2020-06-30 12:11:36|2020-06-30 12:32:43|       member|        0| 3409.736781797531|21.116666666666696|\n",
      "|2020-06-28 14:17:09|2020-06-28 14:27:51|       member|        0|1212.1449337239346|10.700000000000022|\n",
      "+-------------------+-------------------+-------------+---------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate time difference to find the duration in minutes for each trip using .minute, .second and .hour functions\n",
    "ds = ds.withColumn('duration_in_min', (ds.date_diff*24*60) + F.hour(ds.ended_at)*60 + F.minute(ds.ended_at) +\n",
    "                  F.second(ds.ended_at)/60 - F.hour(ds.started_at)*60 - F.minute(ds.started_at) - F.second(ds.started_at)/60)\n",
    "\n",
    "# Confirm the calculation worked\n",
    "ds.select('started_at', 'ended_at', 'member_casual', 'date_diff', 'distance_traveled', 'duration_in_min').show(10)\n",
    "\n",
    "# We can check if the duration of each trip in minutes is correct using started_at and ended_at columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "392d8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary| distance_traveled|    duration_in_min|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|           4068146|            4073182|\n",
      "|   mean|2221.8403058682056| 26.882429780942754|\n",
      "| stddev|2025.9532818899672| 236.67822964689256|\n",
      "|    min|               0.0|-120.30000000000004|\n",
      "|    25%| 865.4421767937478|  7.666666666666614|\n",
      "|    50%|1674.9411822309112| 14.016666666666575|\n",
      "|    75%|3018.4110032739354|              25.85|\n",
      "|    max| 48370.80097108494|           54283.35|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the summary of the calculated fields distance and duration\n",
    "ds.select('distance_traveled', 'duration_in_min').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c545e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------+-------------------+\n",
      "|         started_at|           ended_at|date_diff|    duration_in_min|\n",
      "+-------------------+-------------------+---------+-------------------+\n",
      "|2020-07-25 15:08:21|2020-07-25 13:08:03|        0|-120.30000000000004|\n",
      "|2020-10-16 16:44:52|2020-10-16 15:09:51|        0| -95.01666666666664|\n",
      "|2020-10-16 16:44:53|2020-10-16 15:10:54|        0| -93.98333333333336|\n",
      "|2020-10-16 16:44:55|2020-10-16 15:11:27|        0| -93.46666666666663|\n",
      "|2020-10-16 16:44:56|2020-10-16 15:43:14|        0| -61.69999999999998|\n",
      "|2020-10-16 16:44:58|2020-10-16 15:45:03|        0|-59.916666666666714|\n",
      "|2020-11-01 01:57:30|2020-11-01 01:00:39|        0|             -56.85|\n",
      "|2020-11-01 01:56:26|2020-11-01 01:00:29|        0|-55.949999999999996|\n",
      "|2020-11-01 01:55:57|2020-11-01 01:02:04|        0| -53.88333333333333|\n",
      "|2020-11-01 01:54:40|2020-11-01 01:01:34|        0|-53.099999999999994|\n",
      "+-------------------+-------------------+---------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Duration of a trip can't be negative as seen above, further check by sorting\n",
    "ds.select('started_at', 'ended_at', 'date_diff', 'duration_in_min').sort(ds.duration_in_min.asc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dbb4391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4063030"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove negative observations from duration column\n",
    "ds = ds.filter(col('duration_in_min') >= 0.0)\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89698085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+---------+------------------+------------------+-----------+\n",
      "|         started_at|           ended_at|member_casual|date_diff| distance_traveled|   duration_in_min|day_of_week|\n",
      "+-------------------+-------------------+-------------+---------+------------------+------------------+-----------+\n",
      "|2020-06-13 23:24:48|2020-06-13 23:36:55|       casual|        0| 2248.317583317838|12.116666666666742|   Saturday|\n",
      "|2020-06-26 07:26:10|2020-06-26 07:31:58|       member|        0|1352.5960705541047| 5.799999999999973|     Friday|\n",
      "|2020-06-23 17:12:41|2020-06-23 17:21:14|       member|        0|1143.9287821509456| 8.550000000000015|    Tuesday|\n",
      "|2020-06-20 01:09:35|2020-06-20 01:28:24|       casual|        0|3812.2639415619205|18.816666666666674|   Saturday|\n",
      "|2020-06-25 16:59:25|2020-06-25 17:08:48|       casual|        0|2073.6724042245105| 9.383333333333288|   Thursday|\n",
      "|2020-06-17 18:07:18|2020-06-17 18:18:14|       casual|        0|2073.6724042245105|10.933333333333348|  Wednesday|\n",
      "|2020-06-25 07:24:33|2020-06-25 07:31:11|       member|        0|1352.5960705541047| 6.633333333333337|   Thursday|\n",
      "|2020-06-19 00:00:56|2020-06-19 00:09:15|       casual|        0|1151.4433445280395| 8.316666666666666|     Friday|\n",
      "|2020-06-30 12:11:36|2020-06-30 12:32:43|       member|        0| 3409.736781797531|21.116666666666696|    Tuesday|\n",
      "|2020-06-28 14:17:09|2020-06-28 14:27:51|       member|        0|1212.1449337239346|10.700000000000022|     Sunday|\n",
      "+-------------------+-------------------+-------------+---------+------------------+------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column for day of the week\n",
    "ds = ds.withColumn(\"day_of_week\", date_format(col('started_at'), 'EEEE'))\n",
    "\n",
    "# Confirm the calculation worked\n",
    "ds.select('started_at', 'ended_at', 'member_casual', 'date_diff', 'distance_traveled', 'duration_in_min', 'day_of_week').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d85d646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|member_casual|  count|\n",
      "+-------------+-------+\n",
      "|       casual|1710107|\n",
      "|       member|2352923|\n",
      "|         null|4063030|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Casual vs. member distribution (null represents total number of records)\n",
    "ds.cube('member_casual').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c504f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|day_of_week|  count|\n",
      "+-----------+-------+\n",
      "|  Wednesday| 529720|\n",
      "|   Thursday| 533708|\n",
      "|    Tuesday| 503792|\n",
      "|     Monday| 503884|\n",
      "|     Friday| 597210|\n",
      "|     Sunday| 637741|\n",
      "|   Saturday| 756975|\n",
      "|       null|4063030|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution of day of the week\n",
    "ds.cube('day_of_week').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1fad0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|rideable_type|  count|\n",
      "+-------------+-------+\n",
      "|  docked_bike|2331251|\n",
      "|electric_bike| 888224|\n",
      "| classic_bike| 843555|\n",
      "|         null|4063030|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution of bike types\n",
    "ds.cube('rideable_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "265b55a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o301.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 55.0 failed 1 times, most recent failure: Lost task 10.0 in stage 55.0 (TID 1747) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:684)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:650)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:626)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:583)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:540)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:684)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:650)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:626)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:583)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:540)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9824/3422885553.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcube\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start_station_name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o301.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 55.0 failed 1 times, most recent failure: Lost task 10.0 in stage 55.0 (TID 1747) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:684)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:650)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:626)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:583)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:540)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:188)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:108)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:121)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:162)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:684)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:650)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:626)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:583)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:540)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:175)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "# Start station name frequency is counted and sorted in a descending way\n",
    "df = spark.createDataFrame(ds.cube('start_station_name').count().collect())\n",
    "\n",
    "df.sort(df['count'].desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f8bfed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4063030, 18)\n"
     ]
    }
   ],
   "source": [
    "# Fill null values with 'missing data'\n",
    "ds = ds.na.fill('missing_data')\n",
    "ds = ds.orderBy('started_at')\n",
    "print((ds.count(), len(ds.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e1f9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns to reduce file size of export\n",
    "ds = ds.drop('ride_id')\n",
    "ds = ds.drop('start_station_id')\n",
    "ds = ds.drop('end_station_id')\n",
    "ds = ds.drop('start_lat')\n",
    "ds = ds.drop('end_lat')\n",
    "ds = ds.drop('start_lng')\n",
    "ds = ds.drop('end_lng')\n",
    "ds = ds.drop('a')\n",
    "ds = ds.drop('date_diff')\n",
    "\n",
    "# Export dataset as a .csv file\n",
    "ds.repartition(1).write.csv(r\"C:\\Users\\jeffr\\Documents\\Google Data Analytics\\Case Study 1\\Data - Original\\ds_dropped.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be4f9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
